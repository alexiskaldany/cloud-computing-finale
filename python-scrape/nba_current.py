# -*- coding: utf-8 -*-
"""NBA_Current

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wz8lJ1sntkKNRAA6OsL6NLqX7Ijz17fM

# NBA DATA SCRAPE - CURRENT

## PACKAGE IMPORT / INSTALL
"""

## CAREFUL WITH INSTALL - MAY AFFECT FOLLOWING IMPORTS

#!pip install basketball-reference-scraper==v1.0.3

# LIBRARY IMPORTS
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from scipy import stats as stats
import statistics

import requests
from bs4 import BeautifulSoup
import json
import time
import nltk
import re

from google.colab import drive

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

#from tabulate import tabulate
#from difflib import get_close_matches
#from itertools import islice

print("\nIMPORT SUCCESS")

drive.mount('drive')

"""# TEAM RANKINGS

## URL VARIABLES
"""

# filtered stats of interest

title_links = ['points-per-game', 'average-scoring-margin', 'field-goals-made-per-game', 'field-goals-attempted-per-game',
               'offensive-efficiency', 'defensive-efficiency', 'effective-possession-ratio', 
               'effective-field-goal-pct', 'true-shooting-percentage', 'three-point-pct', 'two-point-pct', 'free-throw-pct',
               'three-pointers-made-per-game', 'three-pointers-attempted-per-game',
               'offensive-rebounds-per-game', 'defensive-rebounds-per-game', 'total-rebounds-per-game',
               'offensive-rebounding-pct', 'defensive-rebounding-pct', 'total-rebounding-percentage',
               'blocks-per-game', 'steals-per-game', 'assists-per-game', 'turnovers-per-game',
               'assist--per--turnover-ratio', 'win-pct-all-games', 'win-pct-close-games', 'possessions-per-game', 'personal-fouls-per-game',
               'opponent-points-per-game', 'opponent-average-scoring-margin', 'opponent-shooting-pct', 'opponent-effective-field-goal-pct',
               'opponent-three-point-pct', 'opponent-two-point-pct', 'opponent-free-throw-pct', 'opponent-true-shooting-percentage',
               'opponent-assists-per-game', 'opponent-turnovers-per-game', 'opponent-assist--per--turnover-ratio',
               'opponent-offensive-rebounds-per-game', 'opponent-defensive-rebounds-per-game', 'opponent-total-rebounds-per-game',
               'opponent-offensive-rebounding-pct', 'opponent-defensive-rebounding-pct',
               'opponent-blocks-per-game', 'opponent-steals-per-game', 'opponent-effective-possession-ratio',

               ]

team_links = ['points-per-game', 'average-scoring-margin',
             'offensive-efficiency', 'percent-of-points-from-2-pointers',
             'percent-of-points-from-3-pointers','percent-of-points-from-free-throws',
             'shooting-pct', 'effective-field-goal-pct', 'true-shooting-percentage', 
             'three-point-pct', 'two-point-pct', 'free-throw-pct',
             'field-goals-made-per-game', 'field-goals-attempted-per-game',
             'three-pointers-made-per-game', 'three-pointers-attempted-per-game',
             'free-throws-made-per-game', 'free-throws-attempted-per-game',
             'three-point-rate', 'fta-per-fga', 'ftm-per-100-possessions',
             'offensive-rebounds-per-game', 'defensive-rebounds-per-game',
             'total-rebounds-per-game',
             'offensive-rebounding-pct', 'defensive-rebounding-pct',
             'total-rebounding-percentage', 'blocks-per-game',
             'steals-per-game',  'assists-per-game',
             'turnovers-per-game', 'assist--per--turnover-ratio',
             'assists-per-fgm', 'games-played', 
              'possessions-per-game', 'extra-chances-per-game',
              'effective-possession-ratio', 
                'win-pct-all-games', 'win-pct-close-games',]
             
opponent_links = ['personal-fouls-per-game'
             'opponent-points-per-game', 'opponent-average-scoring-margin',
             'defensive-efficiency', 'opponent-points-from-2-pointers',
             'opponent-points-from-3-pointers', 'opponent-percent-of-points-from-2-pointers',
             'opponent-percent-of-points-from-3-pointers', 'opponent-percent-of-points-from-free-throws',
             'opponent-shooting-pct', 'opponent-effective-field-goal-pct',
             'opponent-three-point-pct', 'opponent-two-point-pct', 'opponent-free-throw-pct', 'opponent-true-shooting-percentage',
             'opponent-field-goals-made-per-game', 'opponent-field-goals-attempted-per-game', 'opponent-three-pointers-made-per-game',
             'opponent-three-pointers-attempted-per-game', 'opponent-free-throws-made-per-game', 'opponent-free-throws-attempted-per-game',
             'opponent-three-point-rate', 'opponent-two-point-rate', 'opponent-fta-per-fga', 'opponent-ftm-per-100-possessions', 
             'opponent-free-throw-rate', 'opponent-non-blocked-2-pt-pct', 
             'opponent-offensive-rebounds-per-game', 'opponent-defensive-rebounds-per-game', 'opponent-team-rebounds-per-game', 'opponent-total-rebounds-per-game', 'opponent-offensive-rebounding-pct', 'opponent-defensive-rebounding-pct',
             'opponent-blocks-per-game', 'opponent-steals-per-game', 'opponent-block-pct', 'opponent-steals-perpossession',
             'opponent-steal-pct', 'opponent-assists-per-game', 'opponent-turnovers-per-game', 'opponent-assist--per--turnover-ratio',
             'opponent-assists-per-fgm', 'opponent-assists-per-possession', 'opponent-turnovers-per-possession',
             'opponent-turnover-pct', 'opponent-personal-fouls-per-game',
             'opponent-personal-fouls-per-possession', 'opponent-personal-foul-pct',
             'opponent-effective-possession-ratio', 
             'opponent-win-pct-all-games', 'opponent-win-pct-close-games']

# FULL LIST OF LINKS: ['points-per-game', 'average-scoring-margin', 'offensive-efficiency', 'floor-percentage', '1st-half-points-per-game', '2nd-half-points-per-game', 'overtime-points-per-game', 'average-1st-half-margin', 'average-2nd-half-margin', 'average-overtime-margin', 'points-from-2-pointers', 'points-from-3-pointers', 'percent-of-points-from-2-pointers', 'percent-of-points-from-3-pointers', 'percent-of-points-from-free-throws', 'shooting-pct', 'effective-field-goal-pct', 'three-point-pct', 'two-point-pct', 'free-throw-pct', 'true-shooting-percentage', 'field-goals-made-per-game', 'field-goals-attempted-per-game', 'three-pointers-made-per-game', 'three-pointers-attempted-per-game', 'free-throws-made-per-game', 'free-throws-attempted-per-game', 'three-point-rate', 'two-point-rate', 'fta-per-fga', 'ftm-per-100-possessions', 'free-throw-rate', 'non-blocked-2-pt-pct', 'offensive-rebounds-per-game', 'defensive-rebounds-per-game', 'team-rebounds-per-game', 'total-rebounds-per-game', 'offensive-rebounding-pct', 'defensive-rebounding-pct', 'total-rebounding-percentage', 'blocks-per-game', 'steals-per-game', 'block-pct', 'steals-perpossession', 'steal-pct', 'assists-per-game', 'turnovers-per-game', 'turnovers-per-possession', 'assist--per--turnover-ratio', 'assists-per-fgm', 'assists-per-possession', 'turnover-pct', 'personal-fouls-per-game', 'personal-fouls-per-possession', 'personal-foul-pct', 'opponent-points-per-game', 'opponent-average-scoring-margin', 'defensive-efficiency', 'opponent-floor-percentage', 'opponent-1st-half-points-per-game', 'opponent-2nd-half-points-per-game', 'opponent-overtime-points-per-game', 'opponent-points-from-2-pointers', 'opponent-points-from-3-pointers', 'opponent-percent-of-points-from-2-pointers', 'opponent-percent-of-points-from-3-pointers', 'opponent-percent-of-points-from-free-throws', 'opponent-shooting-pct', 'opponent-effective-field-goal-pct', 'opponent-three-point-pct', 'opponent-two-point-pct', 'opponent-free-throw-pct', 'opponent-true-shooting-percentage', 'opponent-field-goals-made-per-game', 'opponent-field-goals-attempted-per-game', 'opponent-three-pointers-made-per-game', 'opponent-three-pointers-attempted-per-game', 'opponent-free-throws-made-per-game', 'opponent-free-throws-attempted-per-game', 'opponent-three-point-rate', 'opponent-two-point-rate', 'opponent-fta-per-fga', 'opponent-ftm-per-100-possessions', 'opponent-free-throw-rate', 'opponent-non-blocked-2-pt-pct', 'opponent-offensive-rebounds-per-game', 'opponent-defensive-rebounds-per-game', 'opponent-team-rebounds-per-game', 'opponent-total-rebounds-per-game', 'opponent-offensive-rebounding-pct', 'opponent-defensive-rebounding-pct', 'opponent-blocks-per-game', 'opponent-steals-per-game', 'opponent-block-pct', 'opponent-steals-perpossession', 'opponent-steal-pct', 'opponent-assists-per-game', 'opponent-turnovers-per-game', 'opponent-assist--per--turnover-ratio', 'opponent-assists-per-fgm', 'opponent-assists-per-possession', 'opponent-turnovers-per-possession', 'opponent-turnover-pct', 'opponent-personal-fouls-per-game', 'opponent-personal-fouls-per-possession', 'opponent-personal-foul-pct', 'games-played', 'possessions-per-game', 'extra-chances-per-game', 'effective-possession-ratio', 'opponent-effective-possession-ratio', 'win-pct-all-games', 'win-pct-close-games', 'opponent-win-pct-all-games', 'opponent-win-pct-close-games']

"""## DATA SCRAPE"""

## TEAMRANKINGS.COM STAT SCRAPE

tr_url = 'https://www.teamrankings.com/nba/stat/'
base_url = 'https://www.teamrankings.com/'

tr_cols = ['Rank', 'Team', '2021', 'Last 3', 'Last 1', 'Home', 'Away', '2020'] #, 'Stat'
tr_link_dict = {link : pd.DataFrame() for link in title_links} #columns=tr_cols
df = pd.DataFrame()

for link in title_links:
  stat_page = requests.get(tr_url + link)
  soup = BeautifulSoup(stat_page.text, 'html.parser')
  table = soup.find_all('table')[0]
  cols = [each.text for each in table.find_all('th')]
  rows = table.find_all('tr')
  for row in rows:
    data = [each.text for each in row.find_all('td')]
    temp_df = pd.DataFrame([data])
      #df = df.append(temp_df, sort=True).reset_index(drop=True)
    tr_link_dict[link] = tr_link_dict[link].append(temp_df, sort=True).reset_index(drop=True)
    tr_link_dict[link] = tr_link_dict[link].dropna()

  tr_link_dict[link].columns = cols
  tr_link_dict[link][link] = tr_link_dict[link]['2021']
  tr_link_dict[link].index = tr_link_dict[link]['Team']
  tr_link_dict[link].drop(columns=['Rank', 'Last 3', 'Last 1', 'Home', 'Away', '2020', '2021', 'Team'], inplace=True)

print(tr_link_dict.keys())

tr_data_hub = pd.DataFrame()

for stat in tr_link_dict:
  #tr_link_dict[stat].replace({'%',''}, regex=True)#.strip('%')
  tr_data_hub[stat] = tr_link_dict[stat]
  #tr_link_dict[stat] = float(tr_link_dict[stat].replace('%',''))

objects = tr_data_hub.select_dtypes(['object'])
tr_data_hub[objects.columns] = objects.apply(lambda x: x.str.strip('%'))

for stat in tr_data_hub:
  tr_data_hub[stat] = pd.to_numeric(tr_data_hub[stat])


#for col in tr_data_hub:
  #tr_data_hub[col] = tr_data_hub[col].astype(float)
  #pd.to_numeric(df['DataFrame Column'],errors='coerce')


#tr_data_hub[stat] = tr_data_hub[stat].replace('%','') #, regex=True
tr_data_hub.head()
#pd.DataFrame.from_dict(tr_link_dict.keys())
#tr_data_hub
#print(tr_link_dict['two-point-pct'])

tr_data_hub.describe()

"""## DATA EXPORT"""

tr_filepath = 'drive/My Drive/GWU/TEAM-7/data/tr_data_hub_3-30-22'

tr_data_hub.to_excel(tr_filepath + '.xlsx', index=True)
tr_data_hub.to_csv(tr_filepath + '.csv', index=True)

"""## DATA IMPORT"""

tr_filepath = 'drive/My Drive/GWU/TEAM-7/data/tr_data_hub_3-30-22'

tr_data_hub = pd.read_excel(tr_filepath + '.xlsx', index_col='Team')
#tr_data_hub = pd.read_csv(tr_filepath + '.csv', index_col='Team')

"""## FEATURE ENGINEERING"""

tr_data_hub.info()

# SCORING MARGIN / POSSESSIONS
tr_data_hub['net-avg-scoring-margin'] = tr_data_hub['average-scoring-margin'] - tr_data_hub['opponent-average-scoring-margin']
tr_data_hub['net-points-per-game'] = tr_data_hub['points-per-game'] - tr_data_hub['opponent-points-per-game']
tr_data_hub['net-effective-possession-ratio'] = tr_data_hub['effective-possession-ratio'] - tr_data_hub['opponent-effective-possession-ratio']
tr_data_hub['net-adj-efficiency'] = tr_data_hub['offensive-efficiency'] - tr_data_hub['defensive-efficiency']

# NET SHOOTING PERCENTAGES
tr_data_hub['net-effective-field-goal-pct'] = tr_data_hub['effective-field-goal-pct'] - tr_data_hub['opponent-effective-field-goal-pct']
tr_data_hub['net-true-shooting-percentage'] = tr_data_hub['true-shooting-percentage'] - tr_data_hub['opponent-true-shooting-percentage']

# STOCKS = STEALS + BLOCKS
tr_data_hub['stocks-per-game'] = tr_data_hub['steals-per-game'] + tr_data_hub['blocks-per-game']
tr_data_hub['opponent-stocks-per-game'] = tr_data_hub['opponent-steals-per-game'] + tr_data_hub['opponent-blocks-per-game']
tr_data_hub['net-stocks-per-game'] = tr_data_hub['stocks-per-game'] - tr_data_hub['opponent-stocks-per-game']

# AST/TO = TURNOVERS / ASSISTS
tr_data_hub['total-turnovers-per-game'] = tr_data_hub['turnovers-per-game'] + tr_data_hub['opponent-turnovers-per-game']
tr_data_hub['net-assist--per--turnover-ratio'] = tr_data_hub['assist--per--turnover-ratio'] - tr_data_hub['opponent-assist--per--turnover-ratio']

# REBOUNDS
tr_data_hub['net-total-rebounds-per-game'] = tr_data_hub['total-rebounds-per-game'] - tr_data_hub['opponent-total-rebounds-per-game']
tr_data_hub['net-off-rebound-pct'] = tr_data_hub['offensive-rebounding-pct'] - tr_data_hub['opponent-offensive-rebounding-pct']
tr_data_hub['net-def-rebound-pct'] = tr_data_hub['defensive-rebounding-pct'] - tr_data_hub['opponent-defensive-rebounding-pct']
  
  # ALTERNATE CALC - yields different performance than above
    #tr_data_hub['net-off-rebound-pct'] = tr_data_hub['offensive-rebounding-pct'] - tr_data_hub['opponent-defensive-rebounding-pct']
    #tr_data_hub['net-def-rebound-pct'] = tr_data_hub['defensive-rebounding-pct'] - tr_data_hub['opponent-offensive-rebounding-pct']

tr_data_hub.info()
#tr_data_hub.columns

"""## FINAL DATA EXPORT"""

tr_data_hub.to_excel(tr_filepath + '.xlsx', index=True)
#tr_data_hub.to_csv(tr_filepath + '.csv', index=True)



"""## FINAL DATA IMPORT"""

tr_filepath = 'drive/My Drive/GWU/TEAM-7/data/tr_data_hub_3-30-22'

tr_data_hub = pd.read_excel(tr_filepath + '.xlsx', index_col='Team')
#tr_data_hub = pd.read_csv(tr_filepath + '.csv', index_col='Team')

"""## DATA VIZ"""

# Create variable for primary features of interest
tr_top_features = ['win-pct-all-games', 'net-adj-efficiency',
                   'net-effective-field-goal-pct',  'net-true-shooting-percentage', 
                   'net-avg-scoring-margin', # 'net-points-per-game', 
                   'net-assist--per--turnover-ratio', 'net-stocks-per-game',]# 'net-effective-possession-ratio',

tr_top_features_df = tr_data_hub[tr_top_features]

team_off_stats = ['win-pct-all-games', 'average-scoring-margin','offensive-efficiency', 'defensive-efficiency',
              'effective-field-goal-pct', 'true-shooting-percentage', 'stocks-per-game',
              'total-rebounding-percentage',
              'assist--per--turnover-ratio',  'effective-possession-ratio']

team_def_stats = ['win-pct-all-games', 'average-scoring-margin' 'defensive-efficiency',
              'opponent-effective-field-goal-pct', 'opponent-true-shooting-percentage', 'opponent-stocks-per-game',
              #'opponent-total-rebounding-percentage',
              'opponent-assist--per--turnover-ratio',  'opponent-effective-possession-ratio']

team_stats_df = tr_data_hub[['win-pct-all-games', 'average-scoring-margin','offensive-efficiency', 'defensive-efficiency',
              'effective-field-goal-pct', 'true-shooting-percentage', 'stocks-per-game',
              'total-rebounding-percentage',
              'assist--per--turnover-ratio',  'effective-possession-ratio']]

team_off_stats_df = tr_data_hub[['win-pct-all-games', 'average-scoring-margin','offensive-efficiency',
              'effective-field-goal-pct', 'true-shooting-percentage', 'stocks-per-game',
              'total-rebounding-percentage',
              'assist--per--turnover-ratio',  'effective-possession-ratio']]

# Features removed or combined to reduce multicollinearity
  # 'offensive-efficiency', 'defensive-efficiency',
  # 'effective-field-goal-pct', 'true-shooting-percentage',
  # 'opponent-effective-field-goal-pct',  'opponent-true-shooting-percentage', 
  # 'points-per-game', 'opponent-points-per-game', 
  # 'stocks-per-game', 'opponent-stocks-per-game', 
  #  'win-pct-close-games', 
  # 'possessions-per-game', 'personal-fouls-per-game',
  # 'field-goals-attempted-per-game', 'three-pointers-attempted-per-game',  'field-goals-made-per-game', 
  # 'opponent-assist--per--turnover-ratio', 'assist--per--turnover-ratio', 
  # 'average-scoring-margin', 'opponent-average-scoring-margin', 'total-turnovers-per-game', 
  # 'blocks-per-game', 'steals-per-game', 'opponent-steals-per-game', 'opponent-blocks-per-game',
  # 'effective-possession-ratio', 'opponent-effective-possession-ratio',
  # 'assists-per-game', 'turnovers-per-game', 'opponent-assists-per-game', 'opponent-turnovers-per-game',
  # 'three-point-pct', 'two-point-pct', 'free-throw-pct', 'three-pointers-made-per-game', 
  # 'opponent-shooting-pct', 'opponent-two-point-pct','opponent-three-point-pct', 'opponent-free-throw-pct',
  # 'net-rebounds-per-game',
  # 'total-rebounding-percentage', 'opponent-offensive-rebounding-pct', 'opponent-defensive-rebounding-pct', # 'net-off-rebound-pct', 'net-def-rebound-pct', 
  # 'offensive-rebounding-pct', 'defensive-rebounding-pct', 'defensive-rebounds-per-game','total-rebounds-per-game', 'offensive-rebounds-per-game', 
  # 'opponent-offensive-rebounds-per-game', 'opponent-defensive-rebounds-per-game', 'opponent-total-rebounds-per-game', 

tr_top_features_df.head() #.reset_index()

# Evaluate range of top features 
tr_top_features_df.describe()
#team_stats_df.describe()

# create correlation variables relative to rest of DataFrame
points_top_corr = tr_data_hub.corr()[['points-per-game']].sort_values(by = 'points-per-game', ascending = False)
#win_pct_top_corr = tr_data_hub[tr_top_features].corr()[['win-pct-all-games']].sort_values(by = 'win-pct-all-games', ascending = False)

win_pct_all_corr = tr_data_hub.corr()[['win-pct-all-games']].sort_values(by = 'win-pct-all-games', ascending = False)
win_pct_close_corr = tr_data_hub.corr()[['win-pct-close-games']].sort_values(by = 'win-pct-close-games', ascending = False)

# generate heatmap to visualize correlation variable
plt.figure(figsize=(8,20))
sns.heatmap(points_top_corr, annot = True, cmap = 'mako', vmin=-1, vmax=1, linecolor = 'white', linewidth = 2);
#sns.heatmap(win_pct_all_corr, annot = True, cmap = 'mako', vmin=-1, vmax=1, linecolor = 'white', linewidth = 2);
#sns.heatmap(win_pct_close_corr, annot = True, cmap = 'flare', vmin=-1, vmax=1, linecolor = 'white', linewidth = 2);

"""## STANDARD SCALER"""

## SCALE DATA
ss = StandardScaler()

tr_fit = ss.fit(tr_top_features_df)
tr_transform = ss.transform(tr_top_features_df)

# HEATMAP (SCALED FEATURES)
plt.figure(figsize=(20,20))
sns.heatmap(tr_transform, xticklabels = tr_top_features_df.columns, yticklabels = tr_top_features_df.index, annot = True, cmap = 'magma', linecolor = 'white', linewidth = 2); #vmin=-1, vmax=1,
#sns.heatmap(win_pct_all_corr, annot = True, cmap = 'mako', vmin=-1, vmax=1, linecolor = 'white', linewidth = 2);
#sns.heatmap(win_pct_close_corr, annot = True, cmap = 'flare', vmin=-1, vmax=1, linecolor = 'white', linewidth = 2);

# SCATTERPLOT - 
plt.figure(figsize=(8,8))
sns.scatterplot(data=tr_data_hub, x='offensive-efficiency', y='defensive-efficiency', palette='mako', markers=True)
plt.title('OFF. VS. DEF. EFFICIENCY (BY CONFERENCE)', fontsize=16)
plt.xlabel('OFFENSIVE EFFICIENCY', fontsize=16)
plt.ylabel('DEFENSIVE EFFICIENCY', fontsize=16)
plt.legend(loc='best')

plt.grid()
plt.tight_layout(pad=1)

for i in tr_data_hub.index:
  plt.text(tr_data_hub['offensive-efficiency'][tr_data_hub.index==i],tr_data_hub['defensive-efficiency'][tr_data_hub.index==i],str(i), color='black')

plt.show();

# SCATTERPLOT - 
plt.figure(figsize=(8,8))
sns.scatterplot(data=tr_data_hub, x='effective-field-goal-pct', y='assist--per--turnover-ratio', palette='mako', markers=True)
plt.title('EFF. FG % VS. ASSIST/TURNOVER %', fontsize=16)
plt.xlabel('EFF. FG %', fontsize=16)
plt.ylabel('ASSIST/TURNOVER %', fontsize=16)
plt.legend(loc='best')

plt.grid()
plt.tight_layout(pad=1)

for i in tr_data_hub.index:
  plt.text(tr_data_hub['effective-field-goal-pct'][tr_data_hub.index==i],tr_data_hub['assist--per--turnover-ratio'][tr_data_hub.index==i],str(i), color='black')

plt.show();

# DFS - SCATTERPLOT - STOCKS
plt.figure(figsize=(8,8))
sns.scatterplot(data=tr_data_hub, x='stocks-per-game', y='opponent-stocks-per-game', palette='mako', markers=True)
plt.title('STOCKS VS. OPP STOCKS / GAME', fontsize=16)
plt.xlabel('STOCKS / GAME', fontsize=16)
plt.ylabel('OPP STOCKS / GAME', fontsize=16)
plt.legend(loc='best')

plt.grid()
plt.tight_layout(pad=1)

for i in tr_data_hub.index:
  plt.text(tr_data_hub['stocks-per-game'][tr_data_hub.index==i]+.01,tr_data_hub['opponent-stocks-per-game'][tr_data_hub.index==i]+.01,str(i), color='black')

plt.show();

# PAIRPLOT
plt.figure(figsize=(16,16))
sns.pairplot(data=tr_top_features_df, palette='mako', markers=True)
#plt.title('PAIRPLOT - TOP CORRELATED FEATURES', fontsize=16)
#plt.xlabel('TBU', fontsize=16)
#plt.ylabel('TBU', fontsize=16)
#plt.legend(loc='best')
#plt.grid()
#plt.tight_layout(pad=1)

plt.show();

"""# DASH"""

tr_data_hub.columns

#|!pip install dash

import dash as dash
from dash import dcc
from dash import html
from dash.dependencies import Input, Output
from dash.exceptions import PreventUpdate

import plotly as ply
import plotly.express as px

external_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']
tr_app = dash.Dash('TEAM RANKINGS APP', external_stylesheets=external_stylesheets)
tr_chart = html.Div([html.H1('NBA DATA - 2022 YTD', style={'textAlign': 'Center'}),
                     html.Br(),
                     html.P('TBU'),
                     dcc.Graph(id='tr_chart'),
                     html.Br(),
                     html.P('STAT A'),
                     dcc.Dropdown(id='stata',
                                  options=[{'label': 'effective-field-goal-pct', 'value': 'effective-field-goal-pct'},
                                           {'label': 'true-shooting-percentage', 'value': 'true-shooting-percentage'},
                                           {'label': 'offensive-efficiency', 'value': 'offensive-efficiency'},
                                           {'label': 'defensive-efficiency', 'value': 'defensive-efficiency'},
                                           {'label': 'net-adj-efficiency', 'value': 'net-adj-efficiency'},
                                           {'label': 'three-pointers-made-per-game', 'value': 'three-pointers-made-per-game'},
                                           {'label': 'three-pointers-attempted-per-game', 'value': 'three-pointers-attempted-per-game'},
                                           {'label': 'assist--per--turnover-ratio', 'value': 'assist--per--turnover-ratio'}], value='defensive-efficiency'),
                     html.Br(),
                     html.P('STAT B'),
                     dcc.Dropdown(id='statb',
                                  options=[{'label': 'effective-field-goal-pct', 'value': 'effective-field-goal-pct'},
                                           {'label': 'true-shooting-percentage', 'value': 'true-shooting-percentage'},
                                           {'label': 'offensive-efficiency', 'value': 'offensive-efficiency'},
                                           {'label': 'defensive-efficiency', 'value': 'defensive-efficiency'},
                                           {'label': 'net-adj-efficiency', 'value': 'net-adj-efficiency'},
                                           {'label': 'three-pointers-made-per-game', 'value': 'three-pointers-made-per-game'},
                                           {'label': 'three-pointers-attempted-per-game', 'value': 'three-pointers-attempted-per-game'},
                                           {'label': 'assist--per--turnover-ratio', 'value': 'assist--per--turnover-ratio'}], value='defensive-efficiency'),
                     html.Br(),
                     ])

@tr_app.callback(Output(component_id='tr-chart', component_property='figure'),
                  #Output(component_id='func2', component_property='children')],
                 [Input(component_id='stata', component_property='value'),
                  Input(component_id='statb', component_property='value')])

def display_chart(stata, statb):
    fig = px.scatter(tr_data_hub, x=[stata], y=[statb])
    return fig

tr_app.run_server(
    port = 8030,
    host = '0.0.0.0',
)

q2_layout = html.Div([html.H1('QUADRATIC FUNCTION', style={'textAlign': 'Center'}),
                             
                             dcc.Graph(id='quad-graph'),
                             html.P('A:'),
                             dcc.Slider(id='a2',
                                        min=-10,
                                        max=10,
                                        value=1,
                                        marks={f'{i}':i for i in range(-10,11)}),
                             html.Br(),
                             html.P('B:'),
                             dcc.Slider(id='b2',
                                        min=-10,
                                        max=10,
                                        value=2,
                                        marks={f'{i}':i for i in range(-10,11)}),
                             html.Br(),
                             html.P('C:'),
                             dcc.Slider(id='c2',
                                        min=-20,
                                        max=0,
                                        value=3,
                                        marks={f'{i}':i for i in range(-20,1)}),
                             html.Br(),
                             html.Br(),
                             html.Br(),
                             ])







def update_q2(a, b, c):
    d = np.sqrt(b**2 - 4*a*c)
    if d < 0:
        raise PreventUpdate
    else:
        y = np.array([a*i**2 + b*i + c for i in x])
        fig = px.line(x=x, y=y)
        disp = f'y(x) = {a}x^2 + {b}x + {c}'
        return fig, disp

"""# MODELING PIPELINE

## IMPORTS
"""

# Commented out IPython magic to ensure Python compatibility.
## LIBRARY IMPORTS

import sys
#from google.colab import drive

import numpy as np
import pandas as pd

# %tensorflow_version 2.x 
import tensorflow as tf
from tensorflow import keras

import warnings
warnings.filterwarnings('ignore')


random_seed = 42
tf.random.set_seed(random_seed)
#import numpy as np
np.random.seed(random_seed)

# Absolute path of current folder
abspath_curr = '/content/drive/My Drive/SPORTS/NBA/'

# Absolute path of shallow utilities folder
abspath_util_shallow = '/content/drive/My Drive/Colab Notebooks/teaching/gwu/machine_learning_I/spring_2022/code/utilities/p2_shallow_learning/'

# IMPORT 

#reg_szn_detail = pd.read_csv('drive/My Drive/SPORTS/NCAAB/data/MRegularSeasonDetailedResults.csv')

#print(tourney_detail.head())
#print('-'*100)
#print(tourney_detail.info())
#print('-'*100)
#print(reg_szn_detail.head())
#print('-'*100)
#print(reg_szn_detail.info())

"""## FEATURE ENGINEERING"""

#reg_szn_detail['WRatio'] = (reg_szn_detail.WCount / (reg_szn_detail.WCount + reg_szn_detail.LCount))

#reg_szn_detail['WtdAvgMargin'] = ((reg_szn_detail['WCount'] * reg_szn_detail['AMarginAvg'] -
#                                reg_szn_detail['LCount'] * reg_szn_detail['BMarginAvg']) /
#                               (reg_szn_detail['WCount'] + reg_szn_detail['LCount'])
                               )
#reg_szn_detail.info()

"""## TARGET"""

# GENERATE COLUMNS FOR TARGET VARIABLE PREDICTION

#reg_szn_detail['ScoreMargin'] = reg_szn_detail['AScore'] - reg_szn_detail['BScore']
#reg_szn_detail['Win'] = (reg_szn_detail['ScoreMargin'] > 0).astype(int)

# DROP COLUMNS
#reg_szn_detail.drop(['NumOT', 'ALoc', 'WCount', 'LCount', 'AMargin', 'BMargin', 'ATeamID', 'BTeamID', 'TeamID', 'DayNum', 'Season'], axis=1, inplace=True)
#reg_szn_detail.info()



"""## TRAIN-TEST"""

# DEFINE TEST FEATURES
test_features = ['ScoreMargin',
                 'Win',
                 'AFGM',
     'BFGM',
     'AFGA',
     'BFGA', 
     'AFGM3',
     'BFGM3',
     'AFGA3',
     'BFGA3',
     'AFTM',
     'BFTM', 
     'AFTA',
     'BFTA',
    'AOR',
    'BOR',
    'ADR',
    'BDR',
     'AAst',
     'BAst',
     'ATO',
     'BTO',
     'AStl',
     'BStl', 
     'ABlk',
     'BBlk',
     'APF',
     'BPF',
     'WtdAvgMargin',
     'WRatio',
]

reg_szn_test = reg_szn_detail[test_features]
reg_szn_test.info()

reg_szn_test.head()

from sklearn.model_selection import train_test_split

train, test = train_test_split(reg_szn_test, train_size=0.8, random_state=42)

# Load the raw training data
df_raw_train = train

# Make a copy of df_raw_train
df_train = df_raw_train.copy(deep=True)

# Load the raw test data
df_raw_test = test

# Make a copy of df_raw_test
df_test = df_raw_test.copy(deep=True)

## TARGET VARIABLE ASSIGNMENT

target = 'Win'

# Print the dimension of df_train
pd.DataFrame([[df_train.shape[0], df_train.shape[1]]], columns=['# rows', '# columns'])

# Print the dimension of df_test
pd.DataFrame([[df_test.shape[0], df_test.shape[1]]], columns=['# rows', '# columns'])

# Print the first 5 rows of df_train
df_train.head()

# Print the first 5 rows of df_test
df_test.head()

"""## SPLIT"""

from sklearn.model_selection import train_test_split

# Divide the training data into training (80%) and validation (20%)
df_train, df_val = train_test_split(df_train, train_size=0.8, random_state=random_seed)

# Reset the index
df_train, df_val = df_train.reset_index(drop=True), df_val.reset_index(drop=True)

# Print the dimension of df_train
pd.DataFrame([[df_train.shape[0], df_train.shape[1]]], columns=['# rows', '# columns'])

# Print the dimension of df_val
pd.DataFrame([[df_val.shape[0], df_val.shape[1]]], columns=['# rows', '# columns'])

# Combine df_train, df_val and df_test
df = pd.concat([df_train, df_val, df_test], sort=False)
df.head()



"""## LABEL ENCODER"""

from sklearn.preprocessing import LabelEncoder

# The LabelEncoder
le = LabelEncoder()

# Encode categorical target in the combined data
df[target] = le.fit_transform(df[target])

# Print the first 5 rows of df
df.head()

"""## PRE-PROCESSING"""

#df.drop(['id'], inplace=True,  axis=1)
#df.head()

# Separating the training data
df_train = df.iloc[:df_train.shape[0], :]

# Separating the validation data
df_val = df.iloc[df_train.shape[0]:df_train.shape[0] + df_val.shape[0], :]

# Separating the test data
df_test = df.iloc[df_train.shape[0] + df_val.shape[0]:, :]

# Print the dimension of df_train
pd.DataFrame([[df_train.shape[0], df_train.shape[1]]], columns=['# rows', '# columns'])

# Print the dimension of df_val
pd.DataFrame([[df_val.shape[0], df_val.shape[1]]], columns=['# rows', '# columns'])

# Print the dimension of df_test
pd.DataFrame([[df_test.shape[0], df_test.shape[1]]], columns=['# rows', '# columns'])

# Combine df_train, df_val and df_test
df = pd.concat([df_train, df_val, df_test], sort=False)

"""## FEATURE MATRIX"""

# Get the feature matrix
X_train = df_train[np.setdiff1d(df_train.columns, [target])].values
X_val = df_val[np.setdiff1d(df_val.columns, [target])].values
X_test = df_test[np.setdiff1d(df_test.columns, [target])].values

# Get the target vector
y_train = df_train[target].values
y_val = df_val[target].values
y_test = df_test[target].values

"""## SCALE"""

from sklearn.preprocessing import MinMaxScaler

# MinMaxScaler
mms = MinMaxScaler()

# Normalize the training data
X_train = mms.fit_transform(X_train)

# Normalize the validation data
X_val = mms.transform(X_val)

# Normalize the test data
X_test = mms.transform(X_test)

"""# HYPERPARAMETER TUNING"""

# Model / Package Imports
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import RandomForestClassifier
#from sklearn.experimental import enable_hist_gradient_boosting
#from sklearn.ensemble import HistGradientBoostingClassifier

# Creating dictionary of the models
models = {'lr': LogisticRegression(class_weight='balanced', random_state=random_seed),
          'mlpc': MLPClassifier(early_stopping=True, random_state=random_seed),
          'rfc': RandomForestClassifier(class_weight='balanced', random_state=random_seed),
          #'hgbc': HistGradientBoostingClassifier(random_state=random_seed)
          }

# Creating the dictionary of the pipelines
from sklearn.pipeline import Pipeline

pipes = {}

for acronym, model in models.items():
    pipes[acronym] = Pipeline([('model', model)])

# Commented out IPython magic to ensure Python compatibility.
## SUPPLEMENTAL - NOT NECESSARY

# Change working directory to the absolute path of the shallow utilities folder
# %cd $abspath_util_shallow

# Import the shallow utitilities
# %run pmlm_utilities_shallow.ipynb

#Getting the predefined split cross-validator
  # feature matrix and target velctor in the combined training and validation data
  # target vector in the combined training and validation data
  # PredefinedSplit
  # See the implementation in pmlm_utilities.ipynb

X_train_val, y_train_val, ps = get_train_val_ps(X_train, y_train, X_val, y_val)

"""## GridSearch CV Param Grids"""

param_grids = {}

"""## Logistic Regression"""

# The parameter grid of tol
tol_grid = [10 ** -5, 10 ** -3] #10 ** -11, 
  # ORIGINAL: tol_grid = [10 ** -5, 10 ** -4, 10 ** -3]

# The parameter grid of C
C_grid = [10, 1, .1, 0.01] # 1 , .001
  # ORIGINAL: C_grid = [0.1, 1, 10]

# The parameter grid of penalty type
penalty_grid = ['none', 'l1', 'l2'] #, 'elasticnet'

# The parameter grid of solver type
solver_grid = ['newton-cg', 'lbfgs', 'liblinear'] # 'sag', 'saga'

# Update param_grids
param_grids['lr'] = [{'model__tol': tol_grid,
                      'model__C': C_grid,
                      #'model__penalty': penalty_grid,
                      'model__solver': solver_grid,
                      }]

"""## MLP Classifier"""

# The grids for alpha
alpha_grids = [10 ** i for i in range(-5, -2)]
  # ORIGINAL: alpha_grids = [10 ** i for i in range(-5, -2)]

# The grids for learning_rate_init
learning_rate_init_grids = [9 ** i for i in range(-3, -1)]
  # ORIGINAL: learning_rate_init_grids = [10 ** i for i in range(-4, -1)]

# Update param_grids
param_grids['mlpc'] = [{'model__alpha': alpha_grids,
                        'model__learning_rate_init': learning_rate_init_grids}]

"""## Random Forest Classifier"""

# The grids for min_samples_split
min_samples_split_grids = [20, 100]
  # ORIGINAL: min_samples_split_grids = [2, 20, 100]

# The grids for min_samples_leaf
min_samples_leaf_grids = [1, 100]
  # ORIGINAL: min_samples_leaf_grids = [1, 20, 100]

# The grids for n_estimators
n_estimators_grids = [100] # 10, 50, 100

# The grids for max_depth
max_depth_grids = [1, 5, 10]

# The grids for max_features
max_features_grids = ['auto', 'sqrt']

# Update param_grids
param_grids['rfc'] = [{'model__min_samples_split': min_samples_split_grids,
                       'model__min_samples_leaf': min_samples_leaf_grids,
                       'model__n_estimators': n_estimators_grids,
                       #'model__max_depth': max_depth_grids,
                       #'model__max_features': max_features_grids,
                       }]

                       # model__max_depth:

"""## GRIDSEARCH"""



import os
# Make directory
directory = os.path.dirname(abspath_curr + '/result/mm2022/GridSearchCV_results/')
if not os.path.exists(directory):
    os.makedirs(directory)

# HYPERPARAMETER TUNING

from sklearn.model_selection import GridSearchCV

# The list of [best_score_, best_params_, best_estimator_] obtained by GridSearchCV
best_score_params_estimator_gs = []

# For each model
for acronym in pipes.keys():
    # GridSearchCV
    gs = GridSearchCV(estimator=pipes[acronym],
                      param_grid=param_grids[acronym],
                      scoring='f1_macro',
                      n_jobs=2, #8
                      cv=ps, #5
                      return_train_score=True)
        
    # Fit the pipeline
    gs = gs.fit(X_train_val, y_train_val)
    
    # Update best_score_params_estimator_gs
    best_score_params_estimator_gs.append([gs.best_score_, gs.best_params_, gs.best_estimator_])
    
    # Sort cv_results in ascending order of 'rank_test_score' and 'std_test_score'
    cv_results = pd.DataFrame.from_dict(gs.cv_results_).sort_values(by=['rank_test_score', 'std_test_score'])
    
    # Get the important columns in cv_results
    important_columns = ['rank_test_score',
                         'mean_test_score', 
                         'std_test_score', 
                         'mean_train_score', 
                         'std_train_score',
                         'mean_fit_time', 
                         'std_fit_time',                        
                         'mean_score_time', 
                         'std_score_time']
    
    # Move the important columns ahead
    cv_results = cv_results[important_columns + sorted(list(set(cv_results.columns) - set(important_columns)))]

    # Write cv_results file
    cv_results.to_csv(path_or_buf=abspath_curr + '/result/mm2022/GridSearchCV_results/' + acronym + '.csv', index=False) #'/result/mnist/cv_results/GridSearchCV/'

# Sort best_score_params_estimator_gs in descending order of the best_score_
best_score_params_estimator_gs = sorted(best_score_params_estimator_gs, key=lambda x : x[0], reverse=True)

# Print best_score_params_estimator_gs
pd.DataFrame(best_score_params_estimator_gs, columns=['best_score', 'best_param', 'best_estimator'])

"""## MODEL SELECTION"""

# Get the best_score, best_params and best_estimator obtained by GridSearchCV
best_score_gs, best_params_gs, best_estimator_gs = best_score_params_estimator_gs[0]

print(best_score_gs)
print(best_params_gs)
print(best_estimator_gs)

"""# GENERATE SUBMISSION FILE
###### Use best model as selected above to generate submission file for Kaggle competition:

## Create Directory
"""

# Make directory
directory = os.path.dirname(abspath_curr + '/result/submission/')
if not os.path.exists(directory):
    os.makedirs(directory)

"""## Generate Submission"""

# Get the prediction on the testing data using best_model
y_test_pred = best_estimator_gs.predict(X_test)

# Transform y_test_pred back to the original class
y_test_pred = le.inverse_transform(y_test_pred)

# Get the submission dataframe
df_submit = pd.DataFrame(np.hstack((np.arange(1, y_test_pred.shape[0] + 1).reshape(-1, 1), y_test_pred.reshape(-1, 1))),
                         columns=['id', target]).astype({'id':int, target:int})                                                                                      

# Generate the submission file
df_submit.to_csv(abspath_curr + '/result/submission/submission.csv', index=False)

"""# INTERPRETATION

## Create Directory
"""

import os

# Make directory
directory = os.path.dirname(abspath_curr + '/result/figure/')
if not os.path.exists(directory):
    os.makedirs(directory)

"""## Feature Importance - Table"""

# Get the best_score, best_param and best_estimator of random forest obtained by GridSearchCV
best_score_rfc, best_param_rfc, best_estimator_rfc = best_score_params_estimator_gs[1]

# Get the dataframe of feature and importance
df_fi_rfc = pd.DataFrame(np.hstack((np.setdiff1d(df.columns, [target]).reshape(-1, 1), best_estimator_rfc.named_steps['model'].feature_importances_.reshape(-1, 1))),
                         columns=['Features', 'Importance'])

# Sort df_fi_rfc in descending order of the importance
df_fi_rfc = df_fi_rfc.sort_values(ascending=False, by='Importance').reset_index(drop=True)

# Print the first 5 rows of df_fi_rfc
df_fi_rfc[:]

"""## Feature Importance - Plot"""

# Create a figure
fig = plt.figure(figsize=(10, 5))

# The bar plot of the top 5 feature importance
plt.bar(df_fi_rfc['Features'][:5], df_fi_rfc['Importance'][:5], color='green')

# Set x-axis
plt.xlabel('Features')
plt.xticks(rotation=90)

# Set y-axis
plt.ylabel('Importance')

# Save and show the figure
plt.tight_layout()
plt.savefig(abspath_curr + '/result/poker/figure/feature_importance_rfc.pdf')
plt.show()

"""# SCRATCH NOTES

## PCA
"""

#%%

X = df[features].values
X = StandardScaler().fit_transform(X)

#%%
pca = PCA(n_components='mle', svd_solver='full') # 'mle'

pca.fit(X)
X_PCA = pca.transform(X)
print('ORIGINAL DIMENSIONS:', X.shape)
print('TRANSFORMED DIMENSIONS:', X_PCA.shape)
print(f'EXPLAINED VARIANCE RATIO: {pca.explained_variance_ratio_}')

#%%
x = np.arange(1, len(np.cumsum(pca.explained_variance_ratio_))+1, 1)

plt.figure(figsize=(12,8))
plt.plot(x, np.cumsum(pca.explained_variance_ratio_))
plt.xticks(x)

plt.show()

"""## SVD"""

# SINGULAR VALUE DECOMPOSITION ANALYSIS [SVD]
# CONDITION NUMBER

# ORIGINAL DATA

from numpy import linalg as LA

H = np.matmul(X.T, X)
_, d, _ = np.linalg.svd(H)
print(f'ORIGINAL DATA: SINGULAR VALUES {d}')
print(f'ORIGINAL DATA: CONDITIONAL NUMBER {LA.cond(X)}')

# TRANSFORMED DATA
H_PCA = np.matmul(X_PCA.T, X_PCA)
_, d_PCA, _ = np.linalg.svd(H_PCA)
print(f'TRANSFORMED DATA: SINGULAR VALUES {d_PCA}')
print(f'TRANSFORMED DATA: CONDITIONAL NUMBER {LA.cond(X_PCA)}')
print('*'*58)

#%%
# CONSTRUCTION OF REDUCED DIMENSION DATASET

#pca_df = pca.explained_variance_ratio_

a, b = X_PCA.shape
column = []

for i in range(b):
    column.append(f'PRINCIPAL COLUMN {i+1}')

df_PCA = pd.DataFrame(data=X_PCA, columns=column)
df_PCA = pd.concat([df_PCA, Y], axis=1)

df_PCA.info()

"""# BASKETBALL REFERENCE

https://github.com/vishaalagartha/basketball_reference_scraper/blob/master/examples.py

## HISTORICAL MATCHUPS

### STAT VARIABLES
"""

# Start Date fixed to reflect modern-day NBA Expansion to 30 teams in 2004

current_date = '2022-03-30'
current_year = 2022

start_2004 = '2003-11-01'
start_2000 = '1999-11-01'
start_2010 = '2009-11-01'


team_code_dict = {'Charlotte Hornets':'CHO', 'Dallas Mavericks':'DAL', 'Denver Nuggets':'DEN',
                  'Houston Rockets':'HOU', 'Los Angeles Clippers':'LAC', 'Miami Heat':'MIA',
                  'New Jersey Nets':'BRK', 'New York Knicks':'NYK', 'San Antonio Spurs':'SAS',
                  'Toronto Raptors':'TOR', 'Utah Jazz':'UTA', 'Vancouver Grizzlies':'MEM',
                  'Washington Wizards':'WAS', 'Boston Celtics':'BOS', 'Chicago Bulls':'CHI',
                  'Cleveland Cavaliers':'CLE', 'Los Angeles Lakers':'LAL', 'Orlando Magic':'ORL',
                  'Portland Trail Blazers':'POR', 'Atlanta Hawks':'ATL', 'Phoenix Suns':'PHO',
                  'Seattle SuperSonics':'OKC', 'Detroit Pistons':'DET', 'Sacramento Kings':'SAC',
                  'Golden State Warriors':'GSW', 'Indiana Pacers':'IND', 'Milwaukee Bucks':'MIL',
                  'Minnesota Timberwolves':'MIN', 'Philadelphia 76ers':'PHI', 'Memphis Grizzlies':'MEM',
                  'New Orleans Hornets':'NOP', 'Charlotte Bobcats':'CHO', 'New Orleans/Oklahoma City Hornets':'NOP',
                  'Oklahoma City Thunder':'OKC', 'Brooklyn Nets':'BRK', 'New Orleans Pelicans':'NOP'
                  }

                  # PENDING CODE CONVERSION / MAPPING:
                    # PHO = PHX
                    # CHO = CHA
                    # BRK = BKN

team_codes = team_code_dict.values()
print(team_codes)

"""### DATA IMPORT"""

matchup_filepath = 'drive/My Drive/GWU/TEAM-7/data/historical_matchups'

matchup_history = pd.read_excel(matchup_filepath + '.xlsx', index_col='DATE')
#matchup_history = pd.read_csv(matchup_filepath + '.csv', index_col='DATE')

"""### PRE-PROCESSING"""

print(matchup_history.columns)
print('-'*100)
print(matchup_history.info())
print('-'*100)
print(matchup_history.head())

matchup_history['HOME'].unique()

matchup_history['VISITOR_CODE'] = matchup_history['VISITOR'].map(team_code_dict)
matchup_history['HOME_CODE'] = matchup_history['HOME'].map(team_code_dict)
matchup_history['VISITOR_MARGIN'] = matchup_history['VISITOR_PTS'] - matchup_history['HOME_PTS']
matchup_history['HOME_MARGIN'] = matchup_history['HOME_PTS'] - matchup_history['VISITOR_PTS']

matchup_history.info()

matchup_history



matchup_history.dropna(inplace=True)
matchup_history.info()







"""### DATA VIZ"""

plt.figure(figsize=(12,10))
sns.histplot(matchup_history['HOME_CODE'], palette='mako') #, rug=True

#plt.title('', fontsize=16)
#plt.xlabel('', fontsize=16)
#plt.ylabel('', fontsize=16)
plt.legend(loc='best')

plt.grid()
plt.tight_layout(pad=1)

plt.show();